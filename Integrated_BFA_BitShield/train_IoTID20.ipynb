{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfc327c",
   "metadata": {},
   "source": [
    "# OBFUS-SIG-NIDS Integration (Obfuscation + SIG-Lite + Bit-Fingerprint)\n",
    "\n",
    "Lưu ý: Chạy các ô này SAU khi đã có `model`, `train_loader`, `test_loader`, `args.DEVICE` trong notebook (sau huấn luyện hoặc sau khi load model).\n",
    "\n",
    "- Tầng A: Obfuscation (permute + inverse) quanh lớp cuối, có `reseed()` runtime.\n",
    "- Tầng B: SIG-Lite (D_KL(u||ŷ) + chuẩn ∂KL/∂W_last, ngưỡng median±k·MAD).\n",
    "- Tầng C: Bit-Fingerprint (PSI histogram int8 + entropy bit-plane, cảnh báo drift).\n",
    "- Controller: gộp alert (OR/AND), cooldown, hành động reseed.\n",
    "\n",
    "Bạn có thể bật/tắt OBFUS-SIG, tinh chỉnh period, k, chuẩn gradient, ngưỡng PSI/entropy ngay trong notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac614476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standalone OBFUS-SIG runtime (inline, no external imports)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Optional, Dict, Tuple, List, Literal\n",
    "\n",
    "# --- Tầng A: Obfuscation ---\n",
    "class ObfusAdapter(nn.Module):\n",
    "    def __init__(self, size: int, dim: int = 1, seed: Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.size = int(size)\n",
    "        perm = self._make_perm(seed)\n",
    "        self.register_buffer(\"perm\", perm, persistent=False)\n",
    "        inv = torch.empty_like(perm)\n",
    "        inv[perm] = torch.arange(self.size, device=perm.device)\n",
    "        self.register_buffer(\"inv_perm\", inv, persistent=False)\n",
    "\n",
    "    def _make_perm(self, seed: Optional[int]) -> torch.Tensor:\n",
    "        gen = torch.Generator()\n",
    "        if seed is not None:\n",
    "            gen.manual_seed(int(seed))\n",
    "        return torch.randperm(self.size, generator=gen)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reseed(self, seed: Optional[int] = None) -> None:\n",
    "        perm = self._make_perm(seed).to(self.perm.device)\n",
    "        self.perm.copy_(perm)\n",
    "        inv = torch.empty_like(self.perm)\n",
    "        inv[self.perm] = torch.arange(self.size, device=self.perm.device)\n",
    "        self.inv_perm.copy_(inv)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, inverse: bool = False) -> torch.Tensor:\n",
    "        index = self.inv_perm if inverse else self.perm\n",
    "        return torch.index_select(x, self.dim, index)\n",
    "\n",
    "class ObfusPair(nn.Module):\n",
    "    def __init__(self, child: nn.Module, size: int, dim: int = 1, seed: Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "        self.child = child\n",
    "        self.adapter = ObfusAdapter(size=size, dim=dim, seed=seed)\n",
    "        self.dim = dim\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reseed(self, seed: Optional[int] = None) -> None:\n",
    "        self.adapter.reseed(seed)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.adapter(x, inverse=False)\n",
    "        y = self.child(x)\n",
    "        y = self.adapter(y, inverse=True)\n",
    "        return y\n",
    "\n",
    "# --- Tầng B: SIG-Lite ---\n",
    "def _median_and_mad(values: torch.Tensor) -> Tuple[float, float]:\n",
    "    med = values.median().item()\n",
    "    mad = (values - values.median()).abs().median().item()\n",
    "    return med, mad\n",
    "\n",
    "def _find_last_linear(model: nn.Module) -> Optional[nn.Linear]:\n",
    "    last = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            last = m\n",
    "    return last\n",
    "\n",
    "class SigLiteMonitor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        probe_loader: torch.utils.data.DataLoader,\n",
    "        period: int = 500,\n",
    "        k: float = 3.0,\n",
    "        device: Optional[torch.device] = None,\n",
    "        grad_norm_type: Literal[\"l1\",\"l2\"] = \"l1\",\n",
    "        normalize_by_params: bool = True,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.probe_loader = probe_loader\n",
    "        self.period = int(period)\n",
    "        self.k = float(k)\n",
    "        self.device = device or next(model.parameters()).device\n",
    "        self.last_layer = _find_last_linear(model)\n",
    "        if self.last_layer is None:\n",
    "            raise ValueError(\"SigLiteMonitor requires a model with a final nn.Linear layer.\")\n",
    "        self._probe_iter = None\n",
    "        self.kl_med = self.kl_mad = self.gn_med = self.gn_mad = None\n",
    "        self.grad_norm_type = grad_norm_type\n",
    "        self.normalize_by_params = normalize_by_params\n",
    "\n",
    "    def _next_probe_batch(self):\n",
    "        if self._probe_iter is None:\n",
    "            self._probe_iter = iter(self.probe_loader)\n",
    "        try:\n",
    "            batch = next(self._probe_iter)\n",
    "        except StopIteration:\n",
    "            self._probe_iter = iter(self.probe_loader)\n",
    "            batch = next(self._probe_iter)\n",
    "        if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "            x, y = batch[0], batch[1]\n",
    "        else:\n",
    "            x, y = batch, None\n",
    "        return x.to(self.device), (y.to(self.device) if y is not None else None)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _probs(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "    def _kl_uniform(self, probs: torch.Tensor) -> torch.Tensor:\n",
    "        c = probs.size(-1)\n",
    "        u = 1.0 / float(c)\n",
    "        eps = 1e-8\n",
    "        log_term = np.log(u + eps) - torch.log(probs + eps)\n",
    "        return (u * log_term).sum(dim=-1).mean()\n",
    "\n",
    "    def _grad_norm(self, kl: torch.Tensor) -> torch.Tensor:\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad = None\n",
    "        grad_w = torch.autograd.grad(kl, self.last_layer.weight, retain_graph=False, allow_unused=False)[0]\n",
    "        if self.grad_norm_type == \"l2\":\n",
    "            norm = torch.linalg.vector_norm(grad_w, ord=2)\n",
    "        else:\n",
    "            norm = grad_w.abs().sum()\n",
    "        if self.normalize_by_params and grad_w.numel() > 0:\n",
    "            norm = norm / float(grad_w.numel())\n",
    "        return norm\n",
    "\n",
    "    def fit_baseline(self, steps: int = 50) -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        kl_vals, gn_vals = [], []\n",
    "        with torch.enable_grad():\n",
    "            for _ in range(steps):\n",
    "                x, _ = self._next_probe_batch()\n",
    "                logits = self.model(x)\n",
    "                probs = self._probs(logits)\n",
    "                kl = self._kl_uniform(probs)\n",
    "                gn = self._grad_norm(kl)\n",
    "                kl_vals.append(kl.detach())\n",
    "                gn_vals.append(gn.detach())\n",
    "        kl_t = torch.stack(kl_vals)\n",
    "        gn_t = torch.stack(gn_vals)\n",
    "        self.kl_med, self.kl_mad = _median_and_mad(kl_t)\n",
    "        self.gn_med, self.gn_mad = _median_and_mad(gn_t)\n",
    "        return {\"kl_median\": self.kl_med, \"kl_mad\": self.kl_mad, \"grad_norm_median\": self.gn_med, \"grad_norm_mad\": self.gn_mad}\n",
    "\n",
    "    def _thresholds(self):\n",
    "        if any(v is None for v in [self.kl_med, self.kl_mad, self.gn_med, self.gn_mad]):\n",
    "            raise RuntimeError(\"Call fit_baseline() before using SigLiteMonitor.\")\n",
    "        kl_L = self.kl_med - self.k * self.kl_mad\n",
    "        kl_U = self.kl_med + self.k * self.kl_mad\n",
    "        gn_L = self.gn_med - self.k * self.gn_mad\n",
    "        gn_U = self.gn_med + self.k * self.gn_mad\n",
    "        return (kl_L, kl_U), (gn_L, gn_U)\n",
    "\n",
    "    def step(self, batch_idx: int) -> Dict[str, float]:\n",
    "        if (batch_idx % self.period) != 0:\n",
    "            return {\"ran\": 0, \"alert\": 0}\n",
    "        self.model.eval()\n",
    "        with torch.enable_grad():\n",
    "            x, _ = self._next_probe_batch()\n",
    "            logits = self.model(x)\n",
    "            probs = self._probs(logits)\n",
    "            kl = self._kl_uniform(probs)\n",
    "            gn = self._grad_norm(kl)\n",
    "        (kl_L, kl_U), (gn_L, gn_U) = self._thresholds()\n",
    "        kl_val = float(kl.detach().cpu().item())\n",
    "        gn_val = float(gn.detach().cpu().item())\n",
    "        kl_alert = int(kl_val < kl_L or kl_val > kl_U)\n",
    "        gn_alert = int(gn_val < gn_L or gn_val > gn_U)\n",
    "        alert = int(kl_alert or gn_alert)\n",
    "        return {\"ran\": 1, \"alert\": alert, \"kl\": kl_val, \"grad_norm_l1\": gn_val, \"kl_L\": kl_L, \"kl_U\": kl_U, \"gn_L\": gn_L, \"gn_U\": gn_U}\n",
    "\n",
    "# --- Tầng C: Bit-Fingerprint ---\n",
    "class BitFingerprint:\n",
    "    def __init__(self, model: nn.Module, threshold_psi: float = 0.1, threshold_entropy: float = 0.15) -> None:\n",
    "        self.model = model\n",
    "        self.threshold_psi = float(threshold_psi)\n",
    "        self.threshold_entropy = float(threshold_entropy)\n",
    "        self.baseline_hists: Dict[str, np.ndarray] = {}\n",
    "        self.baseline_entropy: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _quantize_int8(t: torch.Tensor) -> np.ndarray:\n",
    "        arr = t.detach().float().cpu().numpy()\n",
    "        scale = np.max(np.abs(arr))\n",
    "        if scale <= 1e-12:\n",
    "            scale = 1.0\n",
    "        q = np.clip(np.round(arr / (scale / 127.0)), -128, 127).astype(np.int8)\n",
    "        return q\n",
    "\n",
    "    @staticmethod\n",
    "    def _hist256(q: np.ndarray) -> np.ndarray:\n",
    "        counts, _ = np.histogram(q.astype(np.int16), bins=256, range=(-128, 128))\n",
    "        return counts.astype(np.float64)\n",
    "\n",
    "    @staticmethod\n",
    "    def _psi(p: np.ndarray, q: np.ndarray, eps: float = 1e-8) -> float:\n",
    "        p = p.astype(np.float64); q = q.astype(np.float64)\n",
    "        p = p / (p.sum() + eps)\n",
    "        q = q / (q.sum() + eps)\n",
    "        return float(np.sum((p - q) * np.log((p + eps) / (q + eps))))\n",
    "\n",
    "    @staticmethod\n",
    "    def _bit_plane_entropy(q: np.ndarray) -> np.ndarray:\n",
    "        q_u = q.view(np.uint8)\n",
    "        ent = []\n",
    "        for b in range(8):\n",
    "            bits = (q_u >> b) & 1\n",
    "            p1 = bits.mean(); p0 = 1.0 - p1\n",
    "            eps = 1e-12\n",
    "            if 0.0 < p1 < 1.0:\n",
    "                h = -(p1 * np.log2(p1 + eps) + p0 * np.log2(p0 + eps))\n",
    "            else:\n",
    "                h = 0.0\n",
    "            ent.append(h)\n",
    "        return np.array(ent, dtype=np.float64)\n",
    "\n",
    "    def build_baseline(self) -> Dict[str, float]:\n",
    "        self.baseline_hists.clear(); self.baseline_entropy.clear()\n",
    "        num_params = 0\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.data is None:\n",
    "                continue\n",
    "            q = self._quantize_int8(p.data)\n",
    "            h = self._hist256(q)\n",
    "            self.baseline_hists[name] = h\n",
    "            self.baseline_entropy[name] = self._bit_plane_entropy(q)\n",
    "            num_params += 1\n",
    "        return {\"num_params\": float(num_params)}\n",
    "\n",
    "    def update(self) -> Dict[str, object]:\n",
    "        if not self.baseline_hists:\n",
    "            raise RuntimeError(\"Call build_baseline() before update().\")\n",
    "        psi_per_layer: Dict[str, float] = {}\n",
    "        entropy_drift_per_layer: Dict[str, float] = {}\n",
    "        alert_layers: List[str] = []\n",
    "        entropy_alert_layers: List[str] = []\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if name not in self.baseline_hists:\n",
    "                continue\n",
    "            q = self._quantize_int8(p.data)\n",
    "            h = self._hist256(q)\n",
    "            psi_val = self._psi(self.baseline_hists[name], h)\n",
    "            psi_per_layer[name] = psi_val\n",
    "            if psi_val > self.threshold_psi:\n",
    "                alert_layers.append(name)\n",
    "            cur_ent = self._bit_plane_entropy(q)\n",
    "            base_ent = self.baseline_entropy.get(name, cur_ent)\n",
    "            drift = float(np.max(np.abs(cur_ent - base_ent)))\n",
    "            entropy_drift_per_layer[name] = drift\n",
    "            if drift > self.threshold_entropy:\n",
    "                entropy_alert_layers.append(name)\n",
    "        max_psi = max(psi_per_layer.values()) if psi_per_layer else 0.0\n",
    "        max_entropy_drift = max(entropy_drift_per_layer.values()) if entropy_drift_per_layer else 0.0\n",
    "        any_alert = int(len(alert_layers) > 0 or len(entropy_alert_layers) > 0)\n",
    "        return {\n",
    "            \"alert\": any_alert,\n",
    "            \"max_psi\": float(max_psi),\n",
    "            \"max_entropy_drift\": float(max_entropy_drift),\n",
    "            \"psi_per_layer\": psi_per_layer,\n",
    "            \"alert_layers\": alert_layers,\n",
    "            \"entropy_drift_per_layer\": entropy_drift_per_layer,\n",
    "            \"entropy_alert_layers\": entropy_alert_layers,\n",
    "        }\n",
    "\n",
    "# --- Controller ---\n",
    "class ControllerPolicy:\n",
    "    def __init__(self, alert_mode: str = \"or\", cooldown_steps: int = 1000) -> None:\n",
    "        assert alert_mode in (\"or\", \"and\")\n",
    "        self.alert_mode = alert_mode\n",
    "        self.cooldown_steps = int(cooldown_steps)\n",
    "        self._last_action_step = -10**9\n",
    "        self._step = 0\n",
    "        self._adapters: List[ObfusPair] = []\n",
    "        self._logs: List[Dict[str, object]] = []\n",
    "\n",
    "    def register_adapters(self, adapters: List[ObfusPair]) -> None:\n",
    "        self._adapters = adapters\n",
    "\n",
    "    def _should_act(self, alerts: Dict[str, int]) -> bool:\n",
    "        vals = list(alerts.values())\n",
    "        if not vals:\n",
    "            return False\n",
    "        fire = any(v > 0 for v in vals) if self.alert_mode == \"or\" else all(v > 0 for v in vals)\n",
    "        return fire and (self._step - self._last_action_step >= self.cooldown_steps)\n",
    "\n",
    "    def step(self, metrics: Dict[str, object]) -> Dict[str, object]:\n",
    "        alerts = {\"sig\": int(metrics.get(\"sig_alert\", 0)), \"fp\": int(metrics.get(\"fp_alert\", 0))}\n",
    "        action_taken = \"none\"\n",
    "        if self._should_act(alerts):\n",
    "            for a in self._adapters:\n",
    "                a.reseed()\n",
    "            action_taken = \"reseed_adapters\"\n",
    "            self._last_action_step = self._step\n",
    "        self._logs.append({\"t\": time.time(), \"step\": self._step, \"alerts\": alerts, \"action\": action_taken, \"extras\": metrics})\n",
    "        self._step += 1\n",
    "        return {\"action\": action_taken, \"step\": self._step - 1}\n",
    "\n",
    "# --- Runtime ---\n",
    "class ObfusSigRuntime:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        probe_loader: torch.utils.data.DataLoader,\n",
    "        alert_mode: str = \"or\",\n",
    "        sig_period: int = 500,\n",
    "        sig_k: float = 3.0,\n",
    "        fp_threshold: float = 0.1,\n",
    "        fp_entropy_threshold: float = 0.15,\n",
    "        grad_norm_type: str = \"l1\",\n",
    "        normalize_grad: bool = True,\n",
    "        make_shadow: bool = False,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> None:\n",
    "        self.device = device or next(model.parameters()).device\n",
    "        self.model = model\n",
    "        # Wrap last Linear with obfuscation if present\n",
    "        last_name = None; last_linear = None\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                last_name, last_linear = name, module\n",
    "        if last_linear is not None and last_name is not None:\n",
    "            parent = self.model\n",
    "            path = last_name.split(\".\")\n",
    "            for p in path[:-1]:\n",
    "                parent = getattr(parent, p)\n",
    "            leaf = path[-1]\n",
    "            wrapped = ObfusPair(child=last_linear, size=last_linear.in_features, dim=1, seed=None)\n",
    "            setattr(parent, leaf, wrapped)\n",
    "        # Collect adapters\n",
    "        self.adapters = [m for m in self.model.modules() if isinstance(m, ObfusPair)]\n",
    "        # Monitors\n",
    "        self.sig = SigLiteMonitor(self.model, probe_loader, period=sig_period, k=sig_k, device=self.device, grad_norm_type=grad_norm_type, normalize_by_params=normalize_grad)\n",
    "        self.fp = BitFingerprint(self.model, threshold_psi=fp_threshold, threshold_entropy=fp_entropy_threshold)\n",
    "        self.ctrl = ControllerPolicy(alert_mode=alert_mode, cooldown_steps=max(2, sig_period))\n",
    "        self.ctrl.register_adapters(self.adapters)\n",
    "\n",
    "    def calibrate(self, sig_steps: int = 50) -> Dict[str, float]:\n",
    "        fp_stats = self.fp.build_baseline()\n",
    "        sig_stats = self.sig.fit_baseline(steps=sig_steps)\n",
    "        return {**fp_stats, **sig_stats}\n",
    "\n",
    "    def periodic_check(self, batch_idx: int) -> Dict[str, object]:\n",
    "        sig_ret = self.sig.step(batch_idx)\n",
    "        fp_ret = self.fp.update()\n",
    "        ctrl_ret = self.ctrl.step({\n",
    "            \"sig_alert\": int(sig_ret.get(\"alert\", 0)),\n",
    "            \"fp_alert\": int(fp_ret.get(\"alert\", 0)),\n",
    "            \"sig\": sig_ret,\n",
    "            \"fp\": {\n",
    "                \"max_psi\": fp_ret.get(\"max_psi\", 0.0),\n",
    "                \"max_entropy_drift\": fp_ret.get(\"max_entropy_drift\", 0.0),\n",
    "            },\n",
    "        })\n",
    "        return {\"sig\": sig_ret, \"fp\": fp_ret, \"ctrl\": ctrl_ret}\n",
    "\n",
    "# Xác định thiết bị độc lập\n",
    "try:\n",
    "    device = args.DEVICE\n",
    "except Exception:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0062b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ObfusSigRuntime, calibrate on clean, and evaluate with periodic checks\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "assert 'model' in globals(), 'Cần có biến model (đã huấn luyện hoặc đã load).'\n",
    "assert 'train_loader' in globals() and 'test_loader' in globals(), 'Cần train_loader và test_loader.'\n",
    "\n",
    "# Cấu hình mặc định có thể tinh chỉnh\n",
    "OBFUS_CFG = dict(\n",
    "    alert_mode='or',        # 'or' bắt sớm, 'and' giảm FAR\n",
    "    sig_period=500,\n",
    "    sig_k=3.0,\n",
    "    fp_threshold=0.10,\n",
    "    fp_entropy_threshold=0.15,\n",
    "    grad_norm_type='l1',\n",
    "    normalize_grad=True,\n",
    "    make_shadow=False,\n",
    ")\n",
    "\n",
    "runtime = ObfusSigRuntime(\n",
    "    model=model,\n",
    "    probe_loader=train_loader,\n",
    "    device=device,\n",
    "    **OBFUS_CFG,\n",
    ")\n",
    "cal_stats = runtime.calibrate(sig_steps=50)\n",
    "print('Calibrated:', cal_stats)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_runtime(model, loader, runtime):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Kiểm tra định kỳ (SIG-Lite + Fingerprint + Controller)\n",
    "        runtime.periodic_check(step)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total * 100\n",
    "\n",
    "acc_rt = evaluate_with_runtime(model, test_loader, runtime)\n",
    "print(f'Accuracy (with runtime probe): {acc_rt:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bit-flip attack helpers (PBS/RandomFlip) for quantized models\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Tái sử dụng các lớp lượng tử hoá đã định nghĩa trong notebook: quan_Conv1d, quan_Linear, CustomBlock\n",
    "\n",
    "def _is_quant_module(module):\n",
    "    from types import SimpleNamespace\n",
    "    return isinstance(module, (quan_Conv1d, quan_Linear, CustomBlock))\n",
    "\n",
    "@torch.no_grad()\n",
    "def _flip_one_bit_in_module_weight(module, element_index=None, bit_index=None):\n",
    "    if not hasattr(module, 'weight') or not hasattr(module, 'N_bits') or not hasattr(module, 'step_size'):\n",
    "        return None\n",
    "    weight = module.weight.data\n",
    "    n_bits = int(getattr(module, 'N_bits', 8))\n",
    "    step_size_tensor = getattr(module, 'step_size')\n",
    "    step_size = step_size_tensor.detach().float().view(-1)[0].item() if isinstance(step_size_tensor, torch.Tensor) else float(step_size_tensor)\n",
    "    if step_size == 0.0:\n",
    "        max_abs = weight.abs().max().item() if weight.numel() > 0 else 1.0\n",
    "        step_size = max(1e-6, max_abs / max(1.0, (2 ** n_bits - 2) / 2.0))\n",
    "\n",
    "    flat = weight.view(-1)\n",
    "    numel = flat.numel()\n",
    "    if numel == 0:\n",
    "        return None\n",
    "    if element_index is None:\n",
    "        element_index = int(torch.randint(low=0, high=numel, size=(1,)).item())\n",
    "    if bit_index is None:\n",
    "        bit_index = int(torch.randint(low=0, high=n_bits, size=(1,)).item())\n",
    "\n",
    "    w_int = torch.round(flat / step_size).to(torch.int32)\n",
    "    unsigned = w_int.clone()\n",
    "    neg_mask = unsigned < 0\n",
    "    unsigned[neg_mask] = (1 << n_bits) + unsigned[neg_mask]\n",
    "    toggle_mask = 1 << bit_index\n",
    "    before_unsigned = unsigned[element_index].item()\n",
    "    unsigned[element_index] = (unsigned[element_index].int() ^ toggle_mask)\n",
    "    after_unsigned = unsigned[element_index].item()\n",
    "    mask = (1 << (n_bits - 1)) - 1\n",
    "    signed = -(unsigned & ~mask) + (unsigned & mask)\n",
    "    w_new = signed.to(flat.dtype) * step_size\n",
    "    old_val = flat[element_index].item()\n",
    "    new_val = w_new[element_index].item()\n",
    "    flat.copy_(w_new)\n",
    "    return (old_val, new_val, element_index, bit_index)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _get_quant_modules(model):\n",
    "    modules = []\n",
    "    for name, module in model.named_modules():\n",
    "        if _is_quant_module(module):\n",
    "            modules.append((name, module))\n",
    "    return modules\n",
    "\n",
    "@torch.no_grad()\n",
    "def _random_flip_one_bit(model):\n",
    "    modules = _get_quant_modules(model)\n",
    "    if not modules:\n",
    "        return None\n",
    "    name, module = modules[torch.randint(low=0, high=len(modules), size=(1,)).item()]\n",
    "    result = _flip_one_bit_in_module_weight(module)\n",
    "    return {'module': name, 'result': result}\n",
    "\n",
    "@torch.no_grad()\n",
    "def _compute_batch_loss(model, criterion, batch_x, batch_y):\n",
    "    logits = model(batch_x)\n",
    "    loss = criterion(logits, batch_y)\n",
    "    return float(loss.item())\n",
    "\n",
    "@torch.no_grad()\n",
    "def _progressive_bit_search(model, criterion, calib_x, calib_y, max_trials=16):\n",
    "    modules = _get_quant_modules(model)\n",
    "    if not modules:\n",
    "        return None\n",
    "    base_loss = _compute_batch_loss(model, criterion, calib_x, calib_y)\n",
    "    best = {'delta': 0.0, 'apply': None, 'where': None}\n",
    "    trials = min(max_trials, sum(m.weight.data.numel() > 0 for _, m in modules))\n",
    "    for _ in range(trials):\n",
    "        name, module = modules[torch.randint(low=0, high=len(modules), size=(1,)).item()]\n",
    "        weight = module.weight.data\n",
    "        if weight.numel() == 0:\n",
    "            continue\n",
    "        elem_idx = int(torch.randint(low=0, high=weight.numel(), size=(1,)).item())\n",
    "        bit_idx = int(torch.randint(low=0, high=int(getattr(module, 'N_bits', 8)), size=(1,)).item())\n",
    "        old_val = weight.view(-1)[elem_idx].item()\n",
    "        flip_info = _flip_one_bit_in_module_weight(module, elem_idx, bit_idx)\n",
    "        trial_loss = _compute_batch_loss(model, criterion, calib_x, calib_y)\n",
    "        delta = trial_loss - base_loss\n",
    "        # revert\n",
    "        weight.view(-1)[elem_idx] = torch.tensor(old_val, dtype=weight.dtype, device=weight.device)\n",
    "        if delta > best['delta']:\n",
    "            best = {'delta': delta, 'apply': (name, elem_idx, bit_idx), 'where': flip_info}\n",
    "    if best['apply'] is not None and best['delta'] > 0:\n",
    "        name, elem_idx, bit_idx = best['apply']\n",
    "        for n, m in modules:\n",
    "            if n == name:\n",
    "                flip_result = _flip_one_bit_in_module_weight(m, elem_idx, bit_idx)\n",
    "                return {'module': name, 'elem_idx': elem_idx, 'bit_idx': bit_idx, 'delta_loss': best['delta'], 'result': flip_result}\n",
    "    return _random_flip_one_bit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd117bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack loop with per-iteration logging (integrated with OBFUS-SIG runtime)\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Chọn mode và số vòng lặp\n",
    "ATTACK_MODE = 'pbs'  # 'pbs' | 'random_flip' | 'pbs_to_random' | 'random_to_pbs'\n",
    "ATTACK_ITERS = 25\n",
    "\n",
    "# Chuẩn bị batch hiệu chuẩn cho PBS\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "calib_batch = next(iter(train_loader))\n",
    "calib_x, calib_y = calib_batch[0].to(device), calib_batch[1].to(device)\n",
    "\n",
    "iter_logs = []\n",
    "os.makedirs('results/defense_results', exist_ok=True)\n",
    "\n",
    "for i in range(int(ATTACK_ITERS)):\n",
    "    if ATTACK_MODE == 'pbs':\n",
    "        info = _progressive_bit_search(model, criterion, calib_x, calib_y, max_trials=16)\n",
    "    elif ATTACK_MODE == 'random_flip':\n",
    "        info = _random_flip_one_bit(model)\n",
    "    elif ATTACK_MODE == 'pbs_to_random':\n",
    "        _ = _progressive_bit_search(model, criterion, calib_x, calib_y, max_trials=16)\n",
    "        info = _random_flip_one_bit(model)\n",
    "    elif ATTACK_MODE == 'random_to_pbs':\n",
    "        _ = _random_flip_one_bit(model)\n",
    "        info = _progressive_bit_search(model, criterion, calib_x, calib_y, max_trials=16)\n",
    "    else:\n",
    "        info = _random_flip_one_bit(model)\n",
    "\n",
    "    # Gọi kiểm tra OBFUS-SIG ở bước hiện tại\n",
    "    ret = runtime.periodic_check(i + 1)\n",
    "\n",
    "    # Đánh giá accuracy sau vòng tấn công\n",
    "    acc_i = 0.0\n",
    "    model.eval()\n",
    "    total_i, correct_i = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(1)\n",
    "            correct_i += (pred == y).sum().item()\n",
    "            total_i += y.size(0)\n",
    "    if total_i > 0:\n",
    "        acc_i = correct_i / total_i * 100\n",
    "\n",
    "    # Trích chi tiết flip\n",
    "    module_name = info.get('module') if isinstance(info, dict) else None\n",
    "    old_val = new_val = elem_idx = bit_idx = None\n",
    "    if isinstance(info, dict):\n",
    "        if 'result' in info and info['result'] is not None:\n",
    "            try:\n",
    "                old_val, new_val, elem_idx, bit_idx = info['result']\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            elem_idx = info.get('elem_idx')\n",
    "            bit_idx = info.get('bit_idx')\n",
    "\n",
    "    sig = ret.get('sig', {})\n",
    "    fp = ret.get('fp', {})\n",
    "\n",
    "    iter_logs.append([\n",
    "        i + 1,\n",
    "        ATTACK_MODE,\n",
    "        module_name,\n",
    "        old_val,\n",
    "        new_val,\n",
    "        elem_idx,\n",
    "        bit_idx,\n",
    "        f\"{acc_i:.4f}\",\n",
    "        int(sig.get('alert', 0)),\n",
    "        float(sig.get('kl', 0.0)) if sig.get('ran', 0) else '',\n",
    "        float(sig.get('grad_norm_l1', 0.0)) if sig.get('ran', 0) else '',\n",
    "        int(fp.get('alert', 0)),\n",
    "        float(fp.get('max_psi', 0.0)),\n",
    "        float(fp.get('max_entropy_drift', 0.0)),\n",
    "    ])\n",
    "\n",
    "csv_path = f\"results/defense_results/IoTID20_OBFUS_SIG_{ATTACK_MODE}_iterlog.csv\"\n",
    "with open(csv_path, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        'iteration','mode','module','old_val','new_val','elem_idx','bit_idx',\n",
    "        'accuracy_after_iter','sig_alert','kl','grad_norm','fp_alert','max_psi','max_entropy_drift'\n",
    "    ])\n",
    "    writer.writerows(iter_logs)\n",
    "\n",
    "print('Saved CSV:', csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e05adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override build_model to correct class names\n",
    "class_names = ['mlp','custom1','custom2']\n",
    "\n",
    "def build_model(model_name, in_features, n_classes):\n",
    "    if model_name == 'mlp':\n",
    "        return MLPClassifier(in_features, n_classes)\n",
    "    elif model_name == 'custom1':\n",
    "        # Map to CustomModel (đã định nghĩa ở trên)\n",
    "        return CustomModel(input_size=in_features, output_size=n_classes)\n",
    "    elif model_name == 'custom2':\n",
    "        return CustomModel2(input_size=in_features, output_size=n_classes)\n",
    "    else:\n",
    "        raise ValueError('Unknown model: ' + model_name)\n",
    "\n",
    "print('build_model() has been overridden. Available:', class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d5792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override train/evaluate to adapt input shape for Conv1d models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "def _first_conv1d(module) -> Optional[torch.nn.Conv1d]:\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, torch.nn.Conv1d):\n",
    "            return m\n",
    "    return None\n",
    "\n",
    "def _format_input_for_model(model, x):\n",
    "    # x: [B, features]\n",
    "    conv = _first_conv1d(model)\n",
    "    if conv is None:\n",
    "        return x  # MLP hoặc linear/tabular\n",
    "    if x.dim() == 2:\n",
    "        # Ưu tiên dạng [B, C=features, L=1] nếu in_channels == features\n",
    "        if conv.in_channels == x.size(1):\n",
    "            return x.unsqueeze(-1)  # [B, features, 1]\n",
    "        # Nếu in_channels == 1, coi features là chiều dài\n",
    "        if conv.in_channels == 1:\n",
    "            return x.unsqueeze(1)  # [B, 1, features]\n",
    "    return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device='cpu'):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = _format_input_for_model(model, x)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total * 100\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs, lr, device):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x = _format_input_for_model(model, x)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        \n",
    "        train_acc = correct / total * 100\n",
    "        test_acc = evaluate(model, test_loader, device)\n",
    "        print(f'[Epoch {epoch+1:03d}] Train Acc: {train_acc:6.2f}% | Test Acc: {test_acc:6.2f}% | Loss: {loss_sum/total:.4f}')\n",
    "    \n",
    "    print('\\nHoàn tất huấn luyện!')\n",
    "    return model\n",
    "\n",
    "print('evaluate()/train() overridden with Conv1d input-shape adaptation.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a663e13-ebba-4538-ab96-68592517304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import urllib.request\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f386dc-189d-4c1d-b035-4815734b2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # --- THAM SỐ BẮT BUỘC ---\n",
    "    # !!! THAY ĐỔI ĐƯỜNG DẪN NÀY tới thư mục chứa file CSV của bạn\n",
    "    DATA_ROOT = '../dataset'\n",
    "\n",
    "    MODEL_NAME = 'mlp'  # Lựa chọn giữa 'mlp', 'custom1', 'custom2'\n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE = 1e-3\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    OUTPUT_PATH = 'save/best_model.pth'\n",
    "    \n",
    "    # Các tham số cho việc tải dữ liệu (để trống nếu không dùng)\n",
    "    SOURCE_CSV = None\n",
    "    DOWNLOAD_URL = None\n",
    "\n",
    "args = Config()\n",
    "print(f\"Sử dụng thiết bị: {args.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2ae37-1612-46c1-94f8-72c6cd14a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoTID20CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file, feature_cols=None, label_col='label', scaler=None, label_encoder=None):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if feature_cols is None:\n",
    "            feature_cols = [c for c in df.columns if c != label_col]\n",
    "        self.feature_cols = feature_cols\n",
    "        self.label_col = label_col\n",
    "        X = df[feature_cols].values.astype(np.float32)\n",
    "        y = df[label_col].values\n",
    "\n",
    "        if scaler is None:\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "        else:\n",
    "            X = scaler.transform(X)\n",
    "        self.scaler = scaler\n",
    "\n",
    "        if label_encoder is None:\n",
    "            label_encoder = LabelEncoder()\n",
    "            y = label_encoder.fit_transform(y)\n",
    "        else:\n",
    "            y = label_encoder.transform(y)\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y.astype(np.int64))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def _standardize_and_split_source_csv(data_root, out_train, out_test, source_csv=None):\n",
    "    candidates = []\n",
    "    if source_csv and os.path.isfile(source_csv):\n",
    "        candidates = [source_csv]\n",
    "    else:\n",
    "        candidates = [p for p in glob.glob(os.path.join(data_root, '*.csv')) if os.path.basename(p).lower() not in {'train.csv', 'test.csv'}]\n",
    "    if not candidates:\n",
    "        return False\n",
    "    \n",
    "    print(f\"Đang xử lý file CSV nguồn: {candidates[0]}\")\n",
    "    df = pd.read_csv(candidates[0], skipinitialspace=True)\n",
    "    df = df.drop_duplicates()\n",
    "    for col in ['Flow_ID', 'Src_IP', 'Dst_IP', 'Timestamp', 'Fwd_PSH_Flags', 'Fwd_URG_Flags', 'Fwd_Byts/b_Avg', 'Fwd_Pkts/b_Avg', 'Fwd_Blk_Rate_Avg', 'Bwd_Byts/b_Avg', 'Bwd_Pkts/b_Avg', 'Bwd_Blk_Rate_Avg', 'Init_Fwd_Win_Byts', 'Fwd_Seg_Size_Min']:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df = df.drop_duplicates()\n",
    "    label_col_guess = 'Cat' if 'Cat' in df.columns else ('Label' if 'Label' in df.columns else 'label')\n",
    "    labels = df[[label_col_guess]].copy()\n",
    "    features = df.drop(columns=[c for c in ['Label', 'Cat', 'Sub_Cat'] if c in df.columns], errors='ignore')\n",
    "    \n",
    "    train_df, test_df = train_test_split(pd.concat([features, labels], axis=1), test_size=0.2, random_state=100)\n",
    "    train_df = train_df.rename(columns={label_col_guess: 'label'})\n",
    "    test_df = test_df.rename(columns={label_col_guess: 'label'})\n",
    "    \n",
    "    train_df.to_csv(out_train, index=False)\n",
    "    test_df.to_csv(out_test, index=False)\n",
    "    print(f\"Đã tạo file train.csv và test.csv tại {data_root}\")\n",
    "    return True\n",
    "\n",
    "def _download_csv(download_url: str, dest_path: str) -> bool:\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "        print(f\"Đang tải dữ liệu từ {download_url}...\")\n",
    "        urllib.request.urlretrieve(download_url, dest_path)\n",
    "        print(\"Tải xong!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Tải thất bại: {e}\")\n",
    "        return False\n",
    "\n",
    "def build_iotid20_loaders(data_root, batch_size=256, num_workers=0, source_csv=None, download_url: str = None):\n",
    "    train_csv = os.path.join(data_root, 'train.csv')\n",
    "    test_csv = os.path.join(data_root, 'test.csv')\n",
    "    if not os.path.exists(train_csv) or not os.path.exists(test_csv):\n",
    "        ok = _standardize_and_split_source_csv(data_root, train_csv, test_csv, source_csv=source_csv)\n",
    "        if not ok and download_url:\n",
    "            downloaded = _download_csv(download_url, os.path.join(data_root, 'IoT_Network_Intrusion_Dataset.csv'))\n",
    "            if downloaded:\n",
    "                ok = _standardize_and_split_source_csv(data_root, train_csv, test_csv, source_csv=os.path.join(data_root, 'IoT_Network_Intrusion_Dataset.csv'))\n",
    "        if not ok:\n",
    "            raise FileNotFoundError(f'Không tìm thấy file train.csv/test.csv hoặc file CSV nguồn tại {data_root}.')\n",
    "\n",
    "    label_col = 'label' if 'label' in pd.read_csv(train_csv, nrows=1).columns else 'Cat'\n",
    "    train_ds = IoTID20CSVDataset(train_csv, label_col=label_col)\n",
    "    test_ds = IoTID20CSVDataset(test_csv, feature_cols=train_ds.feature_cols, label_col=label_col, scaler=train_ds.scaler, label_encoder=train_ds.label_encoder)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    n_features = train_ds.X.shape[1]\n",
    "    n_classes = len(train_ds.label_encoder.classes_)\n",
    "    \n",
    "    print(f\"Tạo Dataloaders thành công: {n_features} features, {n_classes} classes.\")\n",
    "    return train_loader, test_loader, n_features, n_classes\n",
    "\n",
    "print(\"Các hàm và lớp xử lý dữ liệu đã sẵn sàng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efddbb7f-62a7-4929-b1dc-234a6e43f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class quan_Linear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(quan_Linear, self).__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.N_bits = 8\n",
    "        self.full_lvls = 2**self.N_bits\n",
    "        self.half_lvls = (self.full_lvls - 2) / 2\n",
    "        # Initialize the step size\n",
    "        self.step_size = nn.Parameter(torch.Tensor([1]), requires_grad=True)\n",
    "        self.__reset_stepsize__()\n",
    "        # flag to enable the inference with quantized weight or self.weight\n",
    "        self.inf_with_weight = False  # disabled by default\n",
    "\n",
    "        # create a vector to identify the weight to each bit\n",
    "        self.b_w = nn.Parameter(2**torch.arange(start=self.N_bits - 1,\n",
    "                                                end=-1,\n",
    "                                                step=-1).unsqueeze(-1).float(),\n",
    "                                requires_grad=False)\n",
    "\n",
    "        self.b_w[0] = -self.b_w[0]  #in-place reverse\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.inf_with_weight:\n",
    "            return F.linear(input, self.weight * self.step_size, self.bias)\n",
    "        else:\n",
    "            self.__reset_stepsize__()\n",
    "            weight_quan = quantize(self.weight, self.step_size,\n",
    "                                   self.half_lvls) * self.step_size\n",
    "            return F.linear(input, weight_quan, self.bias)\n",
    "\n",
    "    def __reset_stepsize__(self):\n",
    "        with torch.no_grad():\n",
    "            self.step_size.data = self.weight.abs().max() / self.half_lvls\n",
    "\n",
    "    def __reset_weight__(self):\n",
    "        '''\n",
    "        This function will reconstruct the weight stored in self.weight.\n",
    "        Replacing the orginal floating-point with the quantized fix-point\n",
    "        weight representation.\n",
    "        '''\n",
    "        # replace the weight with the quantized version\n",
    "        with torch.no_grad():\n",
    "            self.weight.data = quantize(self.weight, self.step_size,\n",
    "                                        self.half_lvls)\n",
    "        # enable the flag, thus now computation does not invovle weight quantization\n",
    "        self.inf_with_weight = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80c5c9-2f19-44eb-a6c0-a8b892cb213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class quan_Conv1d(nn.Conv1d):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 padding=1,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True):\n",
    "        super(quan_Conv1d, self).__init__(in_channels,\n",
    "                                          out_channels,\n",
    "                                          kernel_size,\n",
    "                                          stride=stride,\n",
    "                                          padding=padding,\n",
    "                                          dilation=dilation,\n",
    "                                          groups=groups,\n",
    "                                          bias=bias)\n",
    "\n",
    "        # Số lượng bit để lượng tử hóa trọng số\n",
    "        self.N_bits = 8\n",
    "        self.full_lvls = 2 ** self.N_bits\n",
    "        self.half_lvls = (self.full_lvls - 2) / 2\n",
    "\n",
    "        # Bước lượng tử hóa (step size), là một tham số có thể học được\n",
    "        self.step_size = nn.Parameter(torch.Tensor([1]), requires_grad=True)\n",
    "        self.__reset_stepsize__()\n",
    "\n",
    "        # Cờ để bật hoặc tắt sử dụng trọng số lượng tử hóa\n",
    "        self.inf_with_weight = False  # Tắt theo mặc định\n",
    "\n",
    "        # Tạo một vector để biểu diễn trọng số cho từng bit\n",
    "        self.b_w = nn.Parameter(2 ** torch.arange(start=self.N_bits - 1,\n",
    "                                                  end=-1,\n",
    "                                                  step=-1).unsqueeze(-1).float(),\n",
    "                                requires_grad=False)\n",
    "        self.b_w[0] = -self.b_w[0]  # Biến đổi MSB thành giá trị âm để hỗ trợ bù hai\n",
    "\n",
    "    def __reset_stepsize__(self):\n",
    "        \"\"\"Hàm này dùng để đặt lại giá trị `step_size`.\"\"\"\n",
    "        # Giá trị này có thể được tùy chỉnh tùy thuộc vào yêu cầu của mô hình\n",
    "        self.step_size.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Kiểm tra cờ `inf_with_weight` để quyết định sử dụng trọng số đã lượng tử hóa hay không\n",
    "        if self.inf_with_weight:\n",
    "            quantized_weight = self.quantize_weight(self.weight)\n",
    "            return nn.functional.conv1d(x, quantized_weight, self.bias, self.stride,\n",
    "                                        self.padding, self.dilation, self.groups)\n",
    "        else:\n",
    "            return nn.functional.conv1d(x, self.weight, self.bias, self.stride,\n",
    "                                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def quantize_weight(self, weight):\n",
    "        \"\"\"Lượng tử hóa trọng số theo số bit đã định.\"\"\"\n",
    "        # Tạo trọng số lượng tử hóa bằng cách sử dụng step_size\n",
    "        quantized_weight = torch.round(weight / self.step_size) * self.step_size\n",
    "        quantized_weight = torch.clamp(quantized_weight, -self.half_lvls * self.step_size,\n",
    "                                       (self.half_lvls - 1) * self.step_size)\n",
    "        return quantized_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6161f5-c431-4763-8de4-963d933755af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_quan_bitwidth(model, n_bit):\n",
    "    '''This script change the quantization bit-width of entire model to n_bit'''\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, quan_Conv1d) or isinstance(m, quan_Linear):\n",
    "            m.N_bits = n_bit\n",
    "            # print(\"Change weight bit-width as {}.\".format(m.N_bits))\n",
    "            m.b_w.data = m.b_w.data[-m.N_bits:]\n",
    "            m.b_w[0] = -m.b_w[0]\n",
    "            print(m.b_w)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276445c-d60d-40b2-bd67-af41b9a5086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _quantize_func(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, step_size, half_lvls):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        ctx.step_size = step_size\n",
    "        ctx.half_lvls = half_lvls\n",
    "        output = F.hardtanh(input,\n",
    "                            min_val=-ctx.half_lvls * ctx.step_size.item(),\n",
    "                            max_val=ctx.half_lvls * ctx.step_size.item())\n",
    "\n",
    "        output = torch.round(output / ctx.step_size)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone() / ctx.step_size\n",
    "\n",
    "        return grad_input, None, None\n",
    "\n",
    "quantize = _quantize_func.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7cce9e-cd54-4d48-a6f9-8bb510291db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _bin_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, mu):\n",
    "        ctx.mu = mu\n",
    "        output = input.clone().zero_()\n",
    "        output[input.ge(0)] = 1\n",
    "        output[input.lt(0)] = -1\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone() / ctx.mu\n",
    "        return grad_input, None\n",
    "\n",
    "w_bin = _bin_func.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf2d2b-fe83-4e72-93d9-996cc4a6baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, apply_softmax=False):\n",
    "        super(CustomBlock, self).__init__()\n",
    "        self.N_bits = 16\n",
    "        self.full_lvls = 2 ** self.N_bits\n",
    "        self.half_lvls = (self.full_lvls - 2) / 2\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        # Initialize the step size\n",
    "        self.step_size = nn.Parameter(torch.Tensor([1]), requires_grad=True)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features)) if bias else None\n",
    "\n",
    "        # Reset parameters\n",
    "        self.__reset_stepsize__()\n",
    "        self.reset_parameters()\n",
    "\n",
    "        # Flag for inference with quantized weights\n",
    "        self.inf_with_weight = False\n",
    "\n",
    "        self.b_w = nn.Parameter(2**torch.arange(start=self.N_bits - 1,\n",
    "                                             end=-1,\n",
    "                                             step=-1).unsqueeze(-1).float(),\n",
    "                           requires_grad=False)\n",
    "        self.b_w[0] = -self.b_w[0]  #in-place reverse\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=5 ** 0.5)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.inf_with_weight:\n",
    "            weight_applied = self.weight * self.step_size\n",
    "        else:\n",
    "            self.__reset_stepsize__()\n",
    "            weight_quan = quantize(self.weight, self.step_size, self.half_lvls) * self.step_size\n",
    "            weight_applied = weight_quan\n",
    "\n",
    "        # Linear transformation\n",
    "        input = input.view(input.size(0), -1)  # Flatten input to 2D for matmul\n",
    "        output = input @ weight_applied.T\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        if self.apply_softmax:\n",
    "            output = F.softmax(output, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def __reset_stepsize__(self):\n",
    "        with torch.no_grad():\n",
    "            self.step_size.data = self.weight.abs().max() / self.half_lvls\n",
    "\n",
    "    def __reset_weight__(self):\n",
    "        with torch.no_grad():\n",
    "            self.weight.data = quantize(self.weight, self.step_size, self.half_lvls)\n",
    "        self.inf_with_weight = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8e238-f36f-484f-829f-94bd66f09a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleA(nn.Module):\n",
    "    def __init__(self, nIn, nOut, stride):\n",
    "        super(DownsampleA, self).__init__()\n",
    "        assert stride == 2\n",
    "        self.avg = nn.AvgPool1d(kernel_size=1, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        return torch.cat((x, x.mul(0)), 1)\n",
    "        \n",
    "class SEBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.conv_a = quan_Conv1d(inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_a = nn.BatchNorm1d(planes)\n",
    "        self.dropout_a = nn.Dropout(p=0.3)  # Dropout sau BatchNorm\n",
    "\n",
    "        self.conv_b = quan_Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_b = nn.BatchNorm1d(planes)\n",
    "        self.dropout_b = nn.Dropout(p=0.3)  # Dropout sau BatchNorm\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        basicblock = self.conv_a(x)\n",
    "        basicblock = self.bn_a(basicblock)\n",
    "        basicblock = F.relu(basicblock, inplace=True)\n",
    "        basicblock = self.dropout_a(basicblock)  # Áp dụng dropout\n",
    "\n",
    "        basicblock = self.conv_b(basicblock)\n",
    "        basicblock = self.bn_b(basicblock)\n",
    "        basicblock = self.dropout_b(basicblock)  # Áp dụng dropout\n",
    "\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        return F.relu(residual + basicblock, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60992b5b-2cbc-429f-8972-74a778c8c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_size=69, hidden_sizes=[32, 64, 128, 256, 512], output_size=5):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.fc1 = quan_Conv1d(input_size, hidden_sizes[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.bn_1 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "\n",
    "        self.inplanes = 32\n",
    "        self.stage_1 = self._make_layer(SEBlock, 32, 16, 1)\n",
    "        self.stage_2 = self._make_layer(SEBlock, 64, 16, 2)\n",
    "        self.stage_3 = self._make_layer(SEBlock, 128, 16, 2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.classifier = CustomBlock(128 * SEBlock.expansion, output_size)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                #m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal(m.weight)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        downsample = None\n",
    "        if stride == 2 or self.inplanes != planes * SEBlock.expansion:\n",
    "            downsample = DownsampleA(self.inplanes, planes * SEBlock.expansion, stride) if stride == 2 else None\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride=1, downsample=downsample))\n",
    "        self.inplanes = planes * SEBlock.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.bn_1(x), inplace=True)\n",
    "        \n",
    "        x = self.stage_1(x)\n",
    "        x = self.stage_2(x)\n",
    "        x = self.stage_3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41045f21-d00f-4a61-9e3a-45cec8909891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel2(nn.Module):\n",
    "    def __init__(self, input_size=69, hidden_sizes=[32, 64, 128, 100], output_size=5):\n",
    "        super(CustomModel2, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Conv1d(input_size, hidden_sizes[0], kernel_size=3, stride=2, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.stage_1 = nn.Conv1d(hidden_sizes[0], hidden_sizes[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.stage_2 = nn.Conv1d(hidden_sizes[1], hidden_sizes[2], kernel_size=3, stride=2, padding=1)\n",
    "        self.stage_3 = nn.Conv1d(hidden_sizes[2], hidden_sizes[3], kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Global Pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = CustomBlock(hidden_sizes[-1], output_size, apply_softmax=True)\n",
    "        nn.Dropout(0.15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(self.pool(x))\n",
    "\n",
    "        x = self.stage_1(x)\n",
    "        x = self.activation(self.pool(x))\n",
    "\n",
    "        x = self.stage_2(x)\n",
    "        x = self.activation(self.pool(x))\n",
    "\n",
    "        x = self.stage_3(x)\n",
    "        x = self.activation(self.pool(x))\n",
    "\n",
    "        # Global Pooling\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139ff22-a943-46e4-ab7e-ee22ec8d95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "# Hàm giúp chọn model dựa trên cấu hình\n",
    "def build_model(model_name, in_features, n_classes):\n",
    "    if model_name == 'mlp':\n",
    "        return MLPClassifier(in_features, n_classes)\n",
    "    elif model_name == 'custom1':\n",
    "        return CustomModel1(input_size=in_features, output_size=n_classes)\n",
    "    elif model_name == 'custom2':\n",
    "        return CustomModel2(input_size=in_features, output_size=n_classes)\n",
    "    else:\n",
    "        raise ValueError('Unknown model: ' + model_name)\n",
    "\n",
    "print(\"Các kiến trúc model đã sẵn sàng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7de9c5-4584-4a9f-b739-c9fecd1972f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device='cpu'):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total * 100\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        \n",
    "        train_acc = correct / total * 100\n",
    "        test_acc = evaluate(model, test_loader, device)\n",
    "        print(f'[Epoch {epoch+1:03d}] Train Acc: {train_acc:6.2f}% | Test Acc: {test_acc:6.2f}% | Loss: {loss_sum/total:.4f}')\n",
    "    \n",
    "    print(\"\\nHoàn tất huấn luyện!\")\n",
    "    return model\n",
    "\n",
    "print(\"Các hàm train/evaluate đã sẵn sàng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a68e8-f1f6-4208-a687-96232913de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureCNN(nn.Module):\n",
    "    \"\"\"Pure CNN model without quantization for comparison\"\"\"\n",
    "    def __init__(self, input_size=69, output_size=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers with increasing channels\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Adaptive pooling to handle variable input lengths\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(128, output_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights using Xavier initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle input shape: expect [B, 69] or [B, 69, 1]\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(-1)  # [B, 69] -> [B, 69, 1]\n",
    "\n",
    "        # If input is [B, 69, 1], transpose to [B, 1, 69] for Conv1d\n",
    "        if x.size(-1) == 1:\n",
    "            x = x.transpose(1, 2)  # [B, 69, 1] -> [B, 1, 69]\n",
    "\n",
    "        # Convolutional blocks\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.adaptive_pool(x)  # [B, 512, 1]\n",
    "        x = x.view(x.size(0), -1)  # [B, 512]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.dropout3(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientCNN(nn.Module):\n",
    "    \"\"\"Lightweight CNN with depthwise separable convolutions\"\"\"\n",
    "    def __init__(self, input_size=69, output_size=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Depthwise separable convolution blocks\n",
    "        self.conv1 = nn.Conv1d(input_size, input_size, kernel_size=3, stride=1, padding=1, groups=input_size)\n",
    "        self.pw_conv1 = nn.Conv1d(input_size, 64, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1, groups=64)\n",
    "        self.pw_conv2 = nn.Conv1d(64, 128, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, groups=128)\n",
    "        self.pw_conv3 = nn.Conv1d(128, 256, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Global pooling and classification\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.classifier = nn.Linear(256, output_size)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize with Xavier uniform\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle input shape\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "\n",
    "        if x.size(-1) == 1:\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "        # Depthwise separable blocks\n",
    "        x = F.relu(self.bn1(self.pw_conv1(self.conv1(x))))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.pw_conv2(self.conv2(x))))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.pw_conv3(self.conv3(x))))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Global pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e319c96-473e-4df2-af30-dcc103e97fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Tạo thư mục lưu trữ nếu chưa có\n",
    "os.makedirs(os.path.dirname(args.OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# 2. Chuẩn bị dữ liệu\n",
    "train_loader, test_loader, in_features, n_classes = build_iotid20_loaders(\n",
    "    args.DATA_ROOT, \n",
    "    source_csv=args.SOURCE_CSV, \n",
    "    download_url=args.DOWNLOAD_URL\n",
    ")\n",
    "\n",
    "# 3. Xây dựng model\n",
    "model = build_model(args.MODEL_NAME, in_features, n_classes)\n",
    "print(f\"Đã xây dựng model '{args.MODEL_NAME}'\")\n",
    "print(model)\n",
    "\n",
    "# 4. Bắt đầu huấn luyện\n",
    "trained_model = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader, \n",
    "    test_loader=test_loader, \n",
    "    epochs=args.EPOCHS, \n",
    "    lr=args.LEARNING_RATE, \n",
    "    device=args.DEVICE\n",
    ")\n",
    "\n",
    "# 5. Lưu trọng số của model\n",
    "torch.save(trained_model.state_dict(), args.OUTPUT_PATH)\n",
    "print(f'\\nĐã lưu trọng số vào file: {args.OUTPUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0b6c6-2ff6-4586-a118-ef1953cafe18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
