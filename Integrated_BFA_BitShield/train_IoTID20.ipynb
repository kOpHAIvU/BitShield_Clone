{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a663e13-ebba-4538-ab96-68592517304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import urllib.request\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f386dc-189d-4c1d-b035-4815734b2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # --- THAM SỐ BẮT BUỘC ---\n",
    "    # !!! THAY ĐỔI ĐƯỜNG DẪN NÀY tới thư mục chứa file CSV của bạn\n",
    "    DATA_ROOT = '../dataset'\n",
    "\n",
    "    MODEL_NAME = 'mlp'  # Lựa chọn giữa 'mlp', 'custom1', 'custom2'\n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE = 1e-3\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    OUTPUT_PATH = 'save/best_model.pth'\n",
    "    \n",
    "    # Các tham số cho việc tải dữ liệu (để trống nếu không dùng)\n",
    "    SOURCE_CSV = None\n",
    "    DOWNLOAD_URL = None\n",
    "\n",
    "args = Config()\n",
    "print(f\"Sử dụng thiết bị: {args.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2ae37-1612-46c1-94f8-72c6cd14a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoTID20CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file, feature_cols=None, label_col='label', scaler=None, label_encoder=None):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if feature_cols is None:\n",
    "            feature_cols = [c for c in df.columns if c != label_col]\n",
    "        self.feature_cols = feature_cols\n",
    "        self.label_col = label_col\n",
    "        X = df[feature_cols].values.astype(np.float32)\n",
    "        y = df[label_col].values\n",
    "\n",
    "        if scaler is None:\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "        else:\n",
    "            X = scaler.transform(X)\n",
    "        self.scaler = scaler\n",
    "\n",
    "        if label_encoder is None:\n",
    "            label_encoder = LabelEncoder()\n",
    "            y = label_encoder.fit_transform(y)\n",
    "        else:\n",
    "            y = label_encoder.transform(y)\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y.astype(np.int64))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def _standardize_and_split_source_csv(data_root, out_train, out_test, source_csv=None):\n",
    "    candidates = []\n",
    "    if source_csv and os.path.isfile(source_csv):\n",
    "        candidates = [source_csv]\n",
    "    else:\n",
    "        candidates = [p for p in glob.glob(os.path.join(data_root, '*.csv')) if os.path.basename(p).lower() not in {'train.csv', 'test.csv'}]\n",
    "    if not candidates:\n",
    "        return False\n",
    "    \n",
    "    print(f\"Đang xử lý file CSV nguồn: {candidates[0]}\")\n",
    "    df = pd.read_csv(candidates[0], skipinitialspace=True)\n",
    "    df = df.drop_duplicates()\n",
    "    for col in ['Flow_ID', 'Src_IP', 'Dst_IP', 'Timestamp', 'Fwd_PSH_Flags', 'Fwd_URG_Flags', 'Fwd_Byts/b_Avg', 'Fwd_Pkts/b_Avg', 'Fwd_Blk_Rate_Avg', 'Bwd_Byts/b_Avg', 'Bwd_Pkts/b_Avg', 'Bwd_Blk_Rate_Avg', 'Init_Fwd_Win_Byts', 'Fwd_Seg_Size_Min']:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df = df.drop_duplicates()\n",
    "    label_col_guess = 'Cat' if 'Cat' in df.columns else ('Label' if 'Label' in df.columns else 'label')\n",
    "    labels = df[[label_col_guess]].copy()\n",
    "    features = df.drop(columns=[c for c in ['Label', 'Cat', 'Sub_Cat'] if c in df.columns], errors='ignore')\n",
    "    \n",
    "    train_df, test_df = train_test_split(pd.concat([features, labels], axis=1), test_size=0.2, random_state=100)\n",
    "    train_df = train_df.rename(columns={label_col_guess: 'label'})\n",
    "    test_df = test_df.rename(columns={label_col_guess: 'label'})\n",
    "    \n",
    "    train_df.to_csv(out_train, index=False)\n",
    "    test_df.to_csv(out_test, index=False)\n",
    "    print(f\"Đã tạo file train.csv và test.csv tại {data_root}\")\n",
    "    return True\n",
    "\n",
    "def _download_csv(download_url: str, dest_path: str) -> bool:\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "        print(f\"Đang tải dữ liệu từ {download_url}...\")\n",
    "        urllib.request.urlretrieve(download_url, dest_path)\n",
    "        print(\"Tải xong!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Tải thất bại: {e}\")\n",
    "        return False\n",
    "\n",
    "def build_iotid20_loaders(data_root, batch_size=256, num_workers=0, source_csv=None, download_url: str = None):\n",
    "    train_csv = os.path.join(data_root, 'train.csv')\n",
    "    test_csv = os.path.join(data_root, 'test.csv')\n",
    "    if not os.path.exists(train_csv) or not os.path.exists(test_csv):\n",
    "        ok = _standardize_and_split_source_csv(data_root, train_csv, test_csv, source_csv=source_csv)\n",
    "        if not ok and download_url:\n",
    "            downloaded = _download_csv(download_url, os.path.join(data_root, 'IoT_Network_Intrusion_Dataset.csv'))\n",
    "            if downloaded:\n",
    "                ok = _standardize_and_split_source_csv(data_root, train_csv, test_csv, source_csv=os.path.join(data_root, 'IoT_Network_Intrusion_Dataset.csv'))\n",
    "        if not ok:\n",
    "            raise FileNotFoundError(f'Không tìm thấy file train.csv/test.csv hoặc file CSV nguồn tại {data_root}.')\n",
    "\n",
    "    label_col = 'label' if 'label' in pd.read_csv(train_csv, nrows=1).columns else 'Cat'\n",
    "    train_ds = IoTID20CSVDataset(train_csv, label_col=label_col)\n",
    "    test_ds = IoTID20CSVDataset(test_csv, feature_cols=train_ds.feature_cols, label_col=label_col, scaler=train_ds.scaler, label_encoder=train_ds.label_encoder)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    n_features = train_ds.X.shape[1]\n",
    "    n_classes = len(train_ds.label_encoder.classes_)\n",
    "    \n",
    "    print(f\"Tạo Dataloaders thành công: {n_features} features, {n_classes} classes.\")\n",
    "    return train_loader, test_loader, n_features, n_classes\n",
    "\n",
    "print(\"Các hàm và lớp xử lý dữ liệu đã sẵn sàng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efddbb7f-62a7-4929-b1dc-234a6e43f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class quan_Linear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(quan_Linear, self).__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.N_bits = 8\n",
    "        self.full_lvls = 2**self.N_bits\n",
    "        self.half_lvls = (self.full_lvls - 2) / 2\n",
    "        # Initialize the step size\n",
    "        self.step_size = nn.Parameter(torch.Tensor([1]), requires_grad=True)\n",
    "        self.__reset_stepsize__()\n",
    "        # flag to enable the inference with quantized weight or self.weight\n",
    "        self.inf_with_weight = False  # disabled by default\n",
    "\n",
    "        # create a vector to identify the weight to each bit\n",
    "        self.b_w = nn.Parameter(2**torch.arange(start=self.N_bits - 1,\n",
    "                                                end=-1,\n",
    "                                                step=-1).unsqueeze(-1).float(),\n",
    "                                requires_grad=False)\n",
    "\n",
    "        self.b_w[0] = -self.b_w[0]  #in-place reverse\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.inf_with_weight:\n",
    "            return F.linear(input, self.weight * self.step_size, self.bias)\n",
    "        else:\n",
    "            self.__reset_stepsize__()\n",
    "            weight_quan = quantize(self.weight, self.step_size,\n",
    "                                   self.half_lvls) * self.step_size\n",
    "            return F.linear(input, weight_quan, self.bias)\n",
    "\n",
    "    def __reset_stepsize__(self):\n",
    "        with torch.no_grad():\n",
    "            self.step_size.data = self.weight.abs().max() / self.half_lvls\n",
    "\n",
    "    def __reset_weight__(self):\n",
    "        '''\n",
    "        This function will reconstruct the weight stored in self.weight.\n",
    "        Replacing the orginal floating-point with the quantized fix-point\n",
    "        weight representation.\n",
    "        '''\n",
    "        # replace the weight with the quantized version\n",
    "        with torch.no_grad():\n",
    "            self.weight.data = quantize(self.weight, self.step_size,\n",
    "                                        self.half_lvls)\n",
    "        # enable the flag, thus now computation does not invovle weight quantization\n",
    "        self.inf_with_weight = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80c5c9-2f19-44eb-a6c0-a8b892cb213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class quan_Conv1d(nn.Conv1d):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 padding=1,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True):\n",
    "        super(quan_Conv1d, self).__init__(in_channels,\n",
    "                                          out_channels,\n",
    "                                          kernel_size,\n",
    "                                          stride=stride,\n",
    "                                          padding=padding,\n",
    "                                          dilation=dilation,\n",
    "                                          groups=groups,\n",
    "                                          bias=bias)\n",
    "\n",
    "        # Số lượng bit để lượng tử hóa trọng số\n",
    "        self.N_bits = 8\n",
    "        self.full_lvls = 2 ** self.N_bits\n",
    "        self.half_lvls = (self.full_lvls - 2) / 2\n",
    "\n",
    "        # Bước lượng tử hóa (step size), là một tham số có thể học được\n",
    "        self.step_size = nn.Parameter(torch.Tensor([1]), requires_grad=True)\n",
    "        self.__reset_stepsize__()\n",
    "\n",
    "        # Cờ để bật hoặc tắt sử dụng trọng số lượng tử hóa\n",
    "        self.inf_with_weight = False  # Tắt theo mặc định\n",
    "\n",
    "        # Tạo một vector để biểu diễn trọng số cho từng bit\n",
    "        self.b_w = nn.Parameter(2 ** torch.arange(start=self.N_bits - 1,\n",
    "                                                  end=-1,\n",
    "                                                  step=-1).unsqueeze(-1).float(),\n",
    "                                requires_grad=False)\n",
    "        self.b_w[0] = -self.b_w[0]  # Biến đổi MSB thành giá trị âm để hỗ trợ bù hai\n",
    "\n",
    "    def __reset_stepsize__(self):\n",
    "        \"\"\"Hàm này dùng để đặt lại giá trị `step_size`.\"\"\"\n",
    "        # Giá trị này có thể được tùy chỉnh tùy thuộc vào yêu cầu của mô hình\n",
    "        self.step_size.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Kiểm tra cờ `inf_with_weight` để quyết định sử dụng trọng số đã lượng tử hóa hay không\n",
    "        if self.inf_with_weight:\n",
    "            quantized_weight = self.quantize_weight(self.weight)\n",
    "            return nn.functional.conv1d(x, quantized_weight, self.bias, self.stride,\n",
    "                                        self.padding, self.dilation, self.groups)\n",
    "        else:\n",
    "            return nn.functional.conv1d(x, self.weight, self.bias, self.stride,\n",
    "                                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def quantize_weight(self, weight):\n",
    "        \"\"\"Lượng tử hóa trọng số theo số bit đã định.\"\"\"\n",
    "        # Tạo trọng số lượng tử hóa bằng cách sử dụng step_size\n",
    "        quantized_weight = torch.round(weight / self.step_size) * self.step_size\n",
    "        quantized_weight = torch.clamp(quantized_weight, -self.half_lvls * self.step_size,\n",
    "                                       (self.half_lvls - 1) * self.step_size)\n",
    "        return quantized_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6161f5-c431-4763-8de4-963d933755af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_quan_bitwidth(model, n_bit):\n",
    "    '''This script change the quantization bit-width of entire model to n_bit'''\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, quan_Conv1d) or isinstance(m, quan_Linear):\n",
    "            m.N_bits = n_bit\n",
    "            # print(\"Change weight bit-width as {}.\".format(m.N_bits))\n",
    "            m.b_w.data = m.b_w.data[-m.N_bits:]\n",
    "            m.b_w[0] = -m.b_w[0]\n",
    "            print(m.b_w)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276445c-d60d-40b2-bd67-af41b9a5086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _quantize_func(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, step_size, half_lvls):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        ctx.step_size = step_size\n",
    "        ctx.half_lvls = half_lvls\n",
    "        output = F.hardtanh(input,\n",
    "                            min_val=-ctx.half_lvls * ctx.step_size.item(),\n",
    "                            max_val=ctx.half_lvls * ctx.step_size.item())\n",
    "\n",
    "        output = torch.round(output / ctx.step_size)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone() / ctx.step_size\n",
    "\n",
    "        return grad_input, None, None\n",
    "\n",
    "quantize = _quantize_func.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7cce9e-cd54-4d48-a6f9-8bb510291db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _bin_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, mu):\n",
    "        ctx.mu = mu\n",
    "        output = input.clone().zero_()\n",
    "        output[input.ge(0)] = 1\n",
    "        output[input.lt(0)] = -1\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone() / ctx.mu\n",
    "        return grad_input, None\n",
    "\n",
    "w_bin = _bin_func.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf2d2b-fe83-4e72-93d9-996cc4a6baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, apply_softmax=False):\n",
    "        super(CustomBlock, self).__init__()\n",
    "        self.N_bits = 16\n",
    "        self.full_lvls = 2 ** self.N_bits\n",
    "        self.half_lvls = (self.full_lvls - 2) / 2\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        # Initialize the step size\n",
    "        self.step_size = nn.Parameter(torch.Tensor([1]), requires_grad=True)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features)) if bias else None\n",
    "\n",
    "        # Reset parameters\n",
    "        self.__reset_stepsize__()\n",
    "        self.reset_parameters()\n",
    "\n",
    "        # Flag for inference with quantized weights\n",
    "        self.inf_with_weight = False\n",
    "\n",
    "        self.b_w = nn.Parameter(2**torch.arange(start=self.N_bits - 1,\n",
    "                                             end=-1,\n",
    "                                             step=-1).unsqueeze(-1).float(),\n",
    "                           requires_grad=False)\n",
    "        self.b_w[0] = -self.b_w[0]  #in-place reverse\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=5 ** 0.5)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.inf_with_weight:\n",
    "            weight_applied = self.weight * self.step_size\n",
    "        else:\n",
    "            self.__reset_stepsize__()\n",
    "            weight_quan = quantize(self.weight, self.step_size, self.half_lvls) * self.step_size\n",
    "            weight_applied = weight_quan\n",
    "\n",
    "        # Linear transformation\n",
    "        input = input.view(input.size(0), -1)  # Flatten input to 2D for matmul\n",
    "        output = input @ weight_applied.T\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        if self.apply_softmax:\n",
    "            output = F.softmax(output, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def __reset_stepsize__(self):\n",
    "        with torch.no_grad():\n",
    "            self.step_size.data = self.weight.abs().max() / self.half_lvls\n",
    "\n",
    "    def __reset_weight__(self):\n",
    "        with torch.no_grad():\n",
    "            self.weight.data = quantize(self.weight, self.step_size, self.half_lvls)\n",
    "        self.inf_with_weight = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8e238-f36f-484f-829f-94bd66f09a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleA(nn.Module):\n",
    "    def __init__(self, nIn, nOut, stride):\n",
    "        super(DownsampleA, self).__init__()\n",
    "        assert stride == 2\n",
    "        self.avg = nn.AvgPool1d(kernel_size=1, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        return torch.cat((x, x.mul(0)), 1)\n",
    "        \n",
    "class SEBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.conv_a = quan_Conv1d(inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_a = nn.BatchNorm1d(planes)\n",
    "        self.dropout_a = nn.Dropout(p=0.3)  # Dropout sau BatchNorm\n",
    "\n",
    "        self.conv_b = quan_Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_b = nn.BatchNorm1d(planes)\n",
    "        self.dropout_b = nn.Dropout(p=0.3)  # Dropout sau BatchNorm\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        basicblock = self.conv_a(x)\n",
    "        basicblock = self.bn_a(basicblock)\n",
    "        basicblock = F.relu(basicblock, inplace=True)\n",
    "        basicblock = self.dropout_a(basicblock)  # Áp dụng dropout\n",
    "\n",
    "        basicblock = self.conv_b(basicblock)\n",
    "        basicblock = self.bn_b(basicblock)\n",
    "        basicblock = self.dropout_b(basicblock)  # Áp dụng dropout\n",
    "\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        return F.relu(residual + basicblock, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60992b5b-2cbc-429f-8972-74a778c8c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_size=69, hidden_sizes=[32, 64, 128, 256, 512], output_size=5):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.fc1 = quan_Conv1d(input_size, hidden_sizes[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.bn_1 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "\n",
    "        self.inplanes = 32\n",
    "        self.stage_1 = self._make_layer(SEBlock, 32, 16, 1)\n",
    "        self.stage_2 = self._make_layer(SEBlock, 64, 16, 2)\n",
    "        self.stage_3 = self._make_layer(SEBlock, 128, 16, 2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.classifier = CustomBlock(128 * SEBlock.expansion, output_size)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                n = m.kernel_size[0] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                #m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal(m.weight)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        downsample = None\n",
    "        if stride == 2 or self.inplanes != planes * SEBlock.expansion:\n",
    "            downsample = DownsampleA(self.inplanes, planes * SEBlock.expansion, stride) if stride == 2 else None\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride=1, downsample=downsample))\n",
    "        self.inplanes = planes * SEBlock.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.bn_1(x), inplace=True)\n",
    "        \n",
    "        x = self.stage_1(x)\n",
    "        x = self.stage_2(x)\n",
    "        x = self.stage_3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41045f21-d00f-4a61-9e3a-45cec8909891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel2(nn.Module):\n",
    "    def __init__(self, input_size=69, hidden_sizes=[32, 64, 128, 100], output_size=5):\n",
    "        super(CustomModel2, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Conv1d(input_size, hidden_sizes[0], kernel_size=3, stride=2, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.stage_1 = nn.Conv1d(hidden_sizes[0], hidden_sizes[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.stage_2 = nn.Conv1d(hidden_sizes[1], hidden_sizes[2], kernel_size=3, stride=2, padding=1)\n",
    "        self.stage_3 = nn.Conv1d(hidden_sizes[2], hidden_sizes[3], kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Global Pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = CustomBlock(hidden_sizes[-1], output_size, apply_softmax=True)\n",
    "        nn.Dropout(0.15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(self.pool(x))\n",
    "\n",
    "        x = self.stage_1(x)\n",
    "        x = self.activation(self.pool(x))\n",
    "\n",
    "        x = self.stage_2(x)\n",
    "        x = self.activation(self.pool(x))\n",
    "\n",
    "        x = self.stage_3(x)\n",
    "        x = self.activation(self.pool(x))\n",
    "\n",
    "        # Global Pooling\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139ff22-a943-46e4-ab7e-ee22ec8d95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "# Hàm giúp chọn model dựa trên cấu hình\n",
    "def build_model(model_name, in_features, n_classes):\n",
    "    if model_name == 'mlp':\n",
    "        return MLPClassifier(in_features, n_classes)\n",
    "    elif model_name == 'custom1':\n",
    "        return CustomModel1(input_size=in_features, output_size=n_classes)\n",
    "    elif model_name == 'custom2':\n",
    "        return CustomModel2(input_size=in_features, output_size=n_classes)\n",
    "    else:\n",
    "        raise ValueError('Unknown model: ' + model_name)\n",
    "\n",
    "print(\"Các kiến trúc model đã sẵn sàng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7de9c5-4584-4a9f-b739-c9fecd1972f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device='cpu'):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total * 100\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        \n",
    "        train_acc = correct / total * 100\n",
    "        test_acc = evaluate(model, test_loader, device)\n",
    "        print(f'[Epoch {epoch+1:03d}] Train Acc: {train_acc:6.2f}% | Test Acc: {test_acc:6.2f}% | Loss: {loss_sum/total:.4f}')\n",
    "    \n",
    "    print(\"\\nHoàn tất huấn luyện!\")\n",
    "    return model\n",
    "\n",
    "print(\"Các hàm train/evaluate đã sẵn sàng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a68e8-f1f6-4208-a687-96232913de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureCNN(nn.Module):\n",
    "    \"\"\"Pure CNN model without quantization for comparison\"\"\"\n",
    "    def __init__(self, input_size=69, output_size=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers with increasing channels\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Adaptive pooling to handle variable input lengths\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(128, output_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights using Xavier initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle input shape: expect [B, 69] or [B, 69, 1]\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(-1)  # [B, 69] -> [B, 69, 1]\n",
    "\n",
    "        # If input is [B, 69, 1], transpose to [B, 1, 69] for Conv1d\n",
    "        if x.size(-1) == 1:\n",
    "            x = x.transpose(1, 2)  # [B, 69, 1] -> [B, 1, 69]\n",
    "\n",
    "        # Convolutional blocks\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.adaptive_pool(x)  # [B, 512, 1]\n",
    "        x = x.view(x.size(0), -1)  # [B, 512]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.dropout3(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientCNN(nn.Module):\n",
    "    \"\"\"Lightweight CNN with depthwise separable convolutions\"\"\"\n",
    "    def __init__(self, input_size=69, output_size=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Depthwise separable convolution blocks\n",
    "        self.conv1 = nn.Conv1d(input_size, input_size, kernel_size=3, stride=1, padding=1, groups=input_size)\n",
    "        self.pw_conv1 = nn.Conv1d(input_size, 64, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1, groups=64)\n",
    "        self.pw_conv2 = nn.Conv1d(64, 128, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, groups=128)\n",
    "        self.pw_conv3 = nn.Conv1d(128, 256, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Global pooling and classification\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.classifier = nn.Linear(256, output_size)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize with Xavier uniform\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle input shape\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "\n",
    "        if x.size(-1) == 1:\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "        # Depthwise separable blocks\n",
    "        x = F.relu(self.bn1(self.pw_conv1(self.conv1(x))))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.pw_conv2(self.conv2(x))))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.pw_conv3(self.conv3(x))))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Global pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e319c96-473e-4df2-af30-dcc103e97fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Tạo thư mục lưu trữ nếu chưa có\n",
    "os.makedirs(os.path.dirname(args.OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# 2. Chuẩn bị dữ liệu\n",
    "train_loader, test_loader, in_features, n_classes = build_iotid20_loaders(\n",
    "    args.DATA_ROOT, \n",
    "    source_csv=args.SOURCE_CSV, \n",
    "    download_url=args.DOWNLOAD_URL\n",
    ")\n",
    "\n",
    "# 3. Xây dựng model\n",
    "model = build_model(args.MODEL_NAME, in_features, n_classes)\n",
    "print(f\"Đã xây dựng model '{args.MODEL_NAME}'\")\n",
    "print(model)\n",
    "\n",
    "# 4. Bắt đầu huấn luyện\n",
    "trained_model = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader, \n",
    "    test_loader=test_loader, \n",
    "    epochs=args.EPOCHS, \n",
    "    lr=args.LEARNING_RATE, \n",
    "    device=args.DEVICE\n",
    ")\n",
    "\n",
    "# 5. Lưu trọng số của model\n",
    "torch.save(trained_model.state_dict(), args.OUTPUT_PATH)\n",
    "print(f'\\nĐã lưu trọng số vào file: {args.OUTPUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0b6c6-2ff6-4586-a118-ef1953cafe18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
